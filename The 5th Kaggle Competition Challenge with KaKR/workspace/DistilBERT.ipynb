{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/Kaggle-challenge/workspace/data/'\n",
    "\n",
    "\n",
    "train_d = pd.read_csv(path + 'train.csv')\n",
    "# train_d = pd.read_csv('make_train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "submission = pd.read_csv(path + 'sample_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_na = train_d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>christian</th>\n",
       "      <th>white</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Haven't been here when the oil economy dead ha...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>'\\nBaloney.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Well, when you consider the success of Playboy...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>One more try at this for you and Trid.  4000 y...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>You seem to be ignorant of the fact that the I...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068578</th>\n",
       "      <td>1068578</td>\n",
       "      <td>Don't expect the Alaskan budget problem to go ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068579</th>\n",
       "      <td>1068579</td>\n",
       "      <td>And two priests already on this thread, from w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068580</th>\n",
       "      <td>1068580</td>\n",
       "      <td>Ken_Conklin, You have your views about churche...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068583</th>\n",
       "      <td>1068583</td>\n",
       "      <td>\"The problem appears real, and not confined to...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068584</th>\n",
       "      <td>1068584</td>\n",
       "      <td>The postman was white?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318970 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  toxicity  \\\n",
       "2              2  Haven't been here when the oil economy dead ha...       0.2   \n",
       "3              3                                        '\\nBaloney.       0.0   \n",
       "11            11  Well, when you consider the success of Playboy...       0.0   \n",
       "18            18  One more try at this for you and Trid.  4000 y...       0.0   \n",
       "22            22  You seem to be ignorant of the fact that the I...       0.6   \n",
       "...          ...                                                ...       ...   \n",
       "1068578  1068578  Don't expect the Alaskan budget problem to go ...       0.0   \n",
       "1068579  1068579  And two priests already on this thread, from w...       0.0   \n",
       "1068580  1068580  Ken_Conklin, You have your views about churche...       0.0   \n",
       "1068583  1068583  \"The problem appears real, and not confined to...       0.0   \n",
       "1068584  1068584                             The postman was white?       0.0   \n",
       "\n",
       "         severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "2                    0.0      0.0     0.2     0.0              0.0   \n",
       "3                    0.0      0.0     0.0     0.0              0.0   \n",
       "11                   0.0      0.0     0.0     0.0              0.0   \n",
       "18                   0.0      0.0     0.0     0.0              0.0   \n",
       "22                   0.0      0.0     0.0     0.6              0.2   \n",
       "...                  ...      ...     ...     ...              ...   \n",
       "1068578              0.0      0.0     0.0     0.0              0.0   \n",
       "1068579              0.0      0.0     0.0     0.0              0.0   \n",
       "1068580              0.0      0.0     0.0     0.0              0.0   \n",
       "1068583              0.0      0.0     0.0     0.0              0.0   \n",
       "1068584              0.0      0.0     0.0     0.0              0.0   \n",
       "\n",
       "         sexual_explicit  female      male  christian     white  muslim  \\\n",
       "2                    0.0     0.0  0.000000        0.0  0.000000     0.0   \n",
       "3                    0.0     0.0  0.000000        0.0  0.000000     0.0   \n",
       "11                   0.0     1.0  0.000000        0.0  0.000000     0.0   \n",
       "18                   0.0     0.0  0.000000        0.3  0.000000     0.0   \n",
       "22                   0.0     0.4  0.000000        0.0  0.000000     0.9   \n",
       "...                  ...     ...       ...        ...       ...     ...   \n",
       "1068578              0.0     0.0  0.000000        0.0  0.000000     0.0   \n",
       "1068579              0.0     0.0  0.000000        0.2  0.000000     0.0   \n",
       "1068580              0.0     0.0  0.000000        0.0  0.000000     0.0   \n",
       "1068583              0.0     0.0  0.000000        0.0  0.000000     0.0   \n",
       "1068584              0.0     0.0  0.166667        0.0  0.833333     0.0   \n",
       "\n",
       "         black  homosexual_gay_or_lesbian  jewish  \\\n",
       "2          0.0                        0.0     0.0   \n",
       "3          0.0                        0.0     0.0   \n",
       "11         0.0                        0.0     0.0   \n",
       "18         0.0                        0.0     0.0   \n",
       "22         0.0                        0.0     0.0   \n",
       "...        ...                        ...     ...   \n",
       "1068578    0.0                        0.0     0.0   \n",
       "1068579    0.0                        0.0     0.0   \n",
       "1068580    0.0                        0.0     0.0   \n",
       "1068583    0.0                        0.0     0.0   \n",
       "1068584    0.0                        0.0     0.0   \n",
       "\n",
       "         psychiatric_or_mental_illness  asian  \n",
       "2                                  0.0    0.0  \n",
       "3                                  0.0    0.0  \n",
       "11                                 0.0    0.0  \n",
       "18                                 0.0    0.0  \n",
       "22                                 0.0    0.0  \n",
       "...                                ...    ...  \n",
       "1068578                            0.0    0.0  \n",
       "1068579                            0.0    0.0  \n",
       "1068580                            0.0    0.0  \n",
       "1068583                            0.0    0.0  \n",
       "1068584                            0.0    0.0  \n",
       "\n",
       "[318970 rows x 19 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>christian</th>\n",
       "      <th>white</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No, we need Trump to stay in so he can lose.</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Wow.  Precious.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it hit and run, then the word \"accident\" ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Baloney.\\nThese were GOP that wanted to vote i...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>China is building \"railways in Africa\" ...yep ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068575</th>\n",
       "      <td>1068575</td>\n",
       "      <td>Just so long as the policemen in your argument...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068576</th>\n",
       "      <td>1068576</td>\n",
       "      <td>Wow, not a mention of 8 long years of Obama's ...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068577</th>\n",
       "      <td>1068577</td>\n",
       "      <td>Look at American history and one will see that...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068581</th>\n",
       "      <td>1068581</td>\n",
       "      <td>Too lenient. Considering her crimes, Mansfield...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068582</th>\n",
       "      <td>1068582</td>\n",
       "      <td>Congratulations Elvi!  I know you will do us p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>749615 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  toxicity  \\\n",
       "0              0       No, we need Trump to stay in so he can lose.  0.166667   \n",
       "1              1                                    Wow.  Precious.  0.000000   \n",
       "4              4  When it hit and run, then the word \"accident\" ...  0.000000   \n",
       "5              5  Baloney.\\nThese were GOP that wanted to vote i...  0.000000   \n",
       "6              6  China is building \"railways in Africa\" ...yep ...  0.000000   \n",
       "...          ...                                                ...       ...   \n",
       "1068575  1068575  Just so long as the policemen in your argument...  0.000000   \n",
       "1068576  1068576  Wow, not a mention of 8 long years of Obama's ...  0.166667   \n",
       "1068577  1068577  Look at American history and one will see that...  0.000000   \n",
       "1068581  1068581  Too lenient. Considering her crimes, Mansfield...  0.000000   \n",
       "1068582  1068582  Congratulations Elvi!  I know you will do us p...  0.000000   \n",
       "\n",
       "         severe_toxicity  obscene  threat    insult  identity_attack  \\\n",
       "0                    0.0      0.0     0.0  0.166667              0.0   \n",
       "1                    0.0      0.0     0.0  0.000000              0.0   \n",
       "4                    0.0      0.0     0.0  0.000000              0.0   \n",
       "5                    0.0      0.0     0.0  0.000000              0.0   \n",
       "6                    0.0      0.0     0.0  0.000000              0.0   \n",
       "...                  ...      ...     ...       ...              ...   \n",
       "1068575              0.0      0.0     0.0  0.000000              0.0   \n",
       "1068576              0.0      0.0     0.0  0.166667              0.0   \n",
       "1068577              0.0      0.0     0.0  0.000000              0.0   \n",
       "1068581              0.0      0.0     0.0  0.000000              0.0   \n",
       "1068582              0.0      0.0     0.0  0.000000              0.0   \n",
       "\n",
       "         sexual_explicit  female  male  christian  white  muslim  black  \\\n",
       "0                    0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1                    0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "4                    0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "5                    0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "6                    0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "...                  ...     ...   ...        ...    ...     ...    ...   \n",
       "1068575              0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068576              0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068577              0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068581              0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068582              0.0     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "\n",
       "         homosexual_gay_or_lesbian  jewish  psychiatric_or_mental_illness  \\\n",
       "0                              NaN     NaN                            NaN   \n",
       "1                              NaN     NaN                            NaN   \n",
       "4                              NaN     NaN                            NaN   \n",
       "5                              NaN     NaN                            NaN   \n",
       "6                              NaN     NaN                            NaN   \n",
       "...                            ...     ...                            ...   \n",
       "1068575                        NaN     NaN                            NaN   \n",
       "1068576                        NaN     NaN                            NaN   \n",
       "1068577                        NaN     NaN                            NaN   \n",
       "1068581                        NaN     NaN                            NaN   \n",
       "1068582                        NaN     NaN                            NaN   \n",
       "\n",
       "         asian  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "...        ...  \n",
       "1068575    NaN  \n",
       "1068576    NaN  \n",
       "1068577    NaN  \n",
       "1068581    NaN  \n",
       "1068582    NaN  \n",
       "\n",
       "[749615 rows x 19 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d_excluded = train_d[~train_d.index.isin(train_na.index)]\n",
    "train_d_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068585"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068585"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "749615 + 318970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITY_COLS = ['female',\n",
    "       'male', 'christian', 'white', 'muslim', 'black',\n",
    "       'homosexual_gay_or_lesbian', 'jewish', 'psychiatric_or_mental_illness',\n",
    "       'asian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>christian</th>\n",
       "      <th>white</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068576</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068577</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068582</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>749615 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         female  male  christian  white  muslim  black  \\\n",
       "0           NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1           NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "4           NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "5           NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "6           NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "...         ...   ...        ...    ...     ...    ...   \n",
       "1068575     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068576     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068577     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068581     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068582     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "\n",
       "         homosexual_gay_or_lesbian  jewish  psychiatric_or_mental_illness  \\\n",
       "0                              NaN     NaN                            NaN   \n",
       "1                              NaN     NaN                            NaN   \n",
       "4                              NaN     NaN                            NaN   \n",
       "5                              NaN     NaN                            NaN   \n",
       "6                              NaN     NaN                            NaN   \n",
       "...                            ...     ...                            ...   \n",
       "1068575                        NaN     NaN                            NaN   \n",
       "1068576                        NaN     NaN                            NaN   \n",
       "1068577                        NaN     NaN                            NaN   \n",
       "1068581                        NaN     NaN                            NaN   \n",
       "1068582                        NaN     NaN                            NaN   \n",
       "\n",
       "         asian  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "...        ...  \n",
       "1068575    NaN  \n",
       "1068576    NaN  \n",
       "1068577    NaN  \n",
       "1068581    NaN  \n",
       "1068582    NaN  \n",
       "\n",
       "[749615 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d_excluded = train_d_excluded[IDENTITY_COLS]\n",
    "train_d_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgroup = train_d[(train_d['female'] >= 0.5) | (train_d['male'] >= 0.5) | (train_d['christian'] >= 0.5) | (train_d['white'] >= 0.5) | (train_d['muslim'] >= 0.5) | (train_d['black'] >= 0.5) | (train_d['homosexual_gay_or_lesbian'] >= 0.5) | (train_d['jewish'] >= 0.5) | (train_d['psychiatric_or_mental_illness'] >= 0.5) | (train_d['asian'] >= 0.5)]\n",
    "# SN = subgroup[subgroup['toxicity'] < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background\n",
    "# train_d = train_d[~((train_d['female'] >= 0.5) | (train_d['male'] >= 0.5) | (train_d['christian'] >= 0.5) | (train_d['white'] >= 0.5) | (train_d['muslim'] >= 0.5) | (train_d['black'] >= 0.5) | (train_d['homosexual_gay_or_lesbian'] >= 0.5) | (train_d['jewish'] >= 0.5) | (train_d['psychiatric_or_mental_illness'] >= 0.5) | (train_d['asian'] >= 0.5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_d = pd.concat([train_d, SN],axis=0)\n",
    "# train_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 937492 entries, 0 to 1068583\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   id                             937492 non-null  int64  \n",
      " 1   comment_text                   937492 non-null  object \n",
      " 2   toxicity                       937492 non-null  float64\n",
      " 3   severe_toxicity                937492 non-null  float64\n",
      " 4   obscene                        937492 non-null  float64\n",
      " 5   threat                         937492 non-null  float64\n",
      " 6   insult                         937492 non-null  float64\n",
      " 7   identity_attack                937492 non-null  float64\n",
      " 8   sexual_explicit                937492 non-null  float64\n",
      " 9   female                         187877 non-null  float64\n",
      " 10  male                           187877 non-null  float64\n",
      " 11  christian                      187877 non-null  float64\n",
      " 12  white                          187877 non-null  float64\n",
      " 13  muslim                         187877 non-null  float64\n",
      " 14  black                          187877 non-null  float64\n",
      " 15  homosexual_gay_or_lesbian      187877 non-null  float64\n",
      " 16  jewish                         187877 non-null  float64\n",
      " 17  psychiatric_or_mental_illness  187877 non-null  float64\n",
      " 18  asian                          187877 non-null  float64\n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 143.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 32 #16 \n",
    "EPOCHS = 2 # 1 \n",
    "LEARNING_RATE = 1e-05\n",
    "NUM_WORKERS = 4 # 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', string)\n",
    "    text = re.sub(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def preprocess(df, text_column=\"comment_text\"):\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    # remove url\n",
    "    df[text_column] = df[text_column].str.replace(url_pattern, \" \")\n",
    "\n",
    "    # apply unidecode\n",
    "    df[text_column] = df[text_column].map(unidecode.unidecode)\n",
    "    \n",
    "    # remove emoji\n",
    "    df[text_column] = df[text_column].map(remove_emoji)\n",
    "\n",
    "    # apply lower\n",
    "    df[text_column] = df[text_column].str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_d = preprocess(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train =  train[['comment_text','toxicity']]\n",
    "# train['target'] = (train['toxicity'] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['toxicity', 'severe_toxicity', 'obscene',\n",
    "       'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "value_train = train_d[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068580</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068581</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068582</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068583</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068584</th>\n",
       "      <td>0.045931</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.019249</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068585 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         toxicity  severe_toxicity   obscene    threat    insult  \\\n",
       "0        0.166667         0.000000  0.000000  0.000000  0.166667   \n",
       "1        0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "2        0.200000         0.000000  0.000000  0.200000  0.000000   \n",
       "3        0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "4        0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "...           ...              ...       ...       ...       ...   \n",
       "1068580  0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "1068581  0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "1068582  0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "1068583  0.000000         0.000000  0.000000  0.000000  0.000000   \n",
       "1068584  0.045931         0.000001  0.000161  0.000323  0.005694   \n",
       "\n",
       "         identity_attack  sexual_explicit  \n",
       "0               0.000000         0.000000  \n",
       "1               0.000000         0.000000  \n",
       "2               0.000000         0.000000  \n",
       "3               0.000000         0.000000  \n",
       "4               0.000000         0.000000  \n",
       "...                  ...              ...  \n",
       "1068580         0.000000         0.000000  \n",
       "1068581         0.000000         0.000000  \n",
       "1068582         0.000000         0.000000  \n",
       "1068583         0.000000         0.000000  \n",
       "1068584         0.019249         0.000183  \n",
       "\n",
       "[1068585 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_train.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
      "/tmp/ipykernel_951481/329998477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  value_train[i] = (value_train[i] >= 0.5).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# col = ['toxicity', 'severe_toxicity', 'obscene',\n",
    "#        'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "for i in col:\n",
    "    value_train[i] = (value_train[i] >= 0.5).astype(int)\n",
    "\n",
    "train_d['target'] = value_train.iloc[:, :].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len: int, eval_mode: bool = False):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = dataframe.comment_text\n",
    "        self.eval_mode = eval_mode \n",
    "        if self.eval_mode is False:\n",
    "            self.targets = self.data.target\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text.iloc[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        output = {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "                \n",
    "        if self.eval_mode is False:\n",
    "            output['targets'] = torch.tensor(self.targets.iloc[index], dtype=torch.float)\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
    "training_set = MultiLabelDataset(train_d, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([ 101, 2053, 2057, 2342, 8398, 2000, 2994, 1999, 2061, 2002, 2064, 4558,\n",
       "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': NUM_WORKERS\n",
    "                }\n",
    "training_loader = DataLoader(training_set, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClass(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768, 7)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "0it [00:00, ?it/s]/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.716808021068573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [37:26,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.055100105702877045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [1:14:08,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.04141630604863167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [1:50:52,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.03183456510305405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [2:27:34,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.016457386314868927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [3:04:16,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.07499919831752777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30000it [3:40:59,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.038473259657621384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33394it [4:05:54,  2.26it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.05420756712555885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [36:42,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.049297165125608444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [1:13:26,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.04071388393640518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [1:50:09,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.09505251795053482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [2:26:53,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.010599690489470959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [3:03:38,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.08558698743581772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30000it [3:40:22,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.05465519428253174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33394it [4:05:17,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBGROUP active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Not me - I've seen investigations into trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>She was fully deportable and I am glad someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>That's for the courts to decide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>When she was a scrawny 11-year-old, Sherry Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I cannot help but wonder what Mueller is findi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431410</th>\n",
       "      <td>431410</td>\n",
       "      <td>What would you propose France should do to rid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431411</th>\n",
       "      <td>431411</td>\n",
       "      <td>The article was about Hoodoo, and the criteria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431412</th>\n",
       "      <td>431412</td>\n",
       "      <td>Apparently, according to numerous nationwide w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431413</th>\n",
       "      <td>431413</td>\n",
       "      <td>Obama on the United States as a Christian nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431414</th>\n",
       "      <td>431414</td>\n",
       "      <td>Maybe. On the basis of \"size of problem,\" yeah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431415 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       comment_text\n",
       "0            0  Not me - I've seen investigations into trainin...\n",
       "1            1  She was fully deportable and I am glad someone...\n",
       "2            2                   That's for the courts to decide.\n",
       "3            3  When she was a scrawny 11-year-old, Sherry Joh...\n",
       "4            4  I cannot help but wonder what Mueller is findi...\n",
       "...        ...                                                ...\n",
       "431410  431410  What would you propose France should do to rid...\n",
       "431411  431411  The article was about Hoodoo, and the criteria...\n",
       "431412  431412  Apparently, according to numerous nationwide w...\n",
       "431413  431413  Obama on the United States as a Christian nati...\n",
       "431414  431414  Maybe. On the basis of \"size of problem,\" yeah...\n",
       "\n",
       "[431415 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup = subgroup[['id', 'comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_set = MultiLabelDataset(subgroup, tokenizer, MAX_LEN, eval_mode = True)\n",
    "testing_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 2\n",
    "                }\n",
    "subgroup_loader = DataLoader(subgroup_set, **testing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subgroup_pred = []\n",
    "\n",
    "def sub_t(epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "    \n",
    "        for _, data in tqdm(enumerate(subgroup_loader, 0)):\n",
    "\n",
    "\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            probas = torch.sigmoid(outputs)\n",
    "\n",
    "            all_subgroup_pred.append(probas)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "8194it [10:19, 13.22it/s]\n"
     ]
    }
   ],
   "source": [
    "probas = sub_t(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.4017e-03, 6.0077e-08, 6.7181e-05, 1.5189e-04, 2.8324e-03, 1.6648e-04,\n",
       "         1.6031e-04],\n",
       "        [3.5383e-03, 1.1021e-07, 1.3193e-04, 4.9015e-04, 4.6056e-04, 7.5086e-05,\n",
       "         1.7743e-04],\n",
       "        [1.8004e-02, 3.7344e-07, 1.3199e-04, 2.9430e-04, 6.4860e-03, 2.5748e-03,\n",
       "         1.8874e-04],\n",
       "        [6.3338e-02, 9.7932e-08, 5.4433e-04, 1.8328e-04, 6.0089e-02, 2.9282e-04,\n",
       "         3.4576e-04],\n",
       "        [4.5931e-02, 1.2609e-06, 1.6100e-04, 3.2304e-04, 5.6940e-03, 1.9249e-02,\n",
       "         1.8347e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subgroup_pred = torch.cat(all_subgroup_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subgroup_pred = all_subgroup_pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5704e-02, 2.6573e-07, 4.4558e-04,  ..., 2.3416e-03, 2.2214e-04,\n",
       "         1.7254e-02],\n",
       "        [5.4540e-01, 1.8683e-06, 1.2340e-03,  ..., 4.7111e-01, 1.4231e-02,\n",
       "         5.7379e-04],\n",
       "        [7.6797e-03, 5.2629e-07, 9.7408e-05,  ..., 8.7107e-04, 3.7594e-03,\n",
       "         1.5708e-04],\n",
       "        ...,\n",
       "        [1.8004e-02, 3.7344e-07, 1.3199e-04,  ..., 6.4860e-03, 2.5748e-03,\n",
       "         1.8874e-04],\n",
       "        [6.3338e-02, 9.7932e-08, 5.4433e-04,  ..., 6.0089e-02, 2.9282e-04,\n",
       "         3.4576e-04],\n",
       "        [4.5931e-02, 1.2609e-06, 1.6100e-04,  ..., 5.6940e-03, 1.9249e-02,\n",
       "         1.8347e-04]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subgroup_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['toxicity', 'severe_toxicity', 'obscene',\n",
    "       'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "for i,name in enumerate(col):\n",
    "    subgroup[name] = all_subgroup_pred[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Well, when you consider the success of Playboy...</td>\n",
       "      <td>0.015704</td>\n",
       "      <td>2.657304e-07</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.017254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>You seem to be ignorant of the fact that the I...</td>\n",
       "      <td>0.545399</td>\n",
       "      <td>1.868301e-06</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.471114</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>What do Christians have to do with this articl...</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>5.262926e-07</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>Hillary will meet the requirements of her fund...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>1.169475e-07</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>Many couples incorporate the above mentioned \"...</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>4.219486e-07</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.021162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068545</th>\n",
       "      <td>1068545</td>\n",
       "      <td>In a recent highly publicised case, the man in...</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>6.007705e-08</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068546</th>\n",
       "      <td>1068546</td>\n",
       "      <td>https://www.youtube.com/watch?v=Qup47XJGIl4\\n\\...</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>1.102081e-07</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068556</th>\n",
       "      <td>1068556</td>\n",
       "      <td>Liu Xiaobo is too famous. If he was able to tr...</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>3.734395e-07</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068572</th>\n",
       "      <td>1068572</td>\n",
       "      <td>Well aren't you our Debbie Downer? You must be...</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>9.793220e-08</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.060089</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068584</th>\n",
       "      <td>1068584</td>\n",
       "      <td>The postman was white?</td>\n",
       "      <td>0.045931</td>\n",
       "      <td>1.260926e-06</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.019249</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131093 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  toxicity  \\\n",
       "11            11  Well, when you consider the success of Playboy...  0.015704   \n",
       "22            22  You seem to be ignorant of the fact that the I...  0.545399   \n",
       "37            37  What do Christians have to do with this articl...  0.007680   \n",
       "59            59  Hillary will meet the requirements of her fund...  0.000512   \n",
       "66            66  Many couples incorporate the above mentioned \"...  0.009281   \n",
       "...          ...                                                ...       ...   \n",
       "1068545  1068545  In a recent highly publicised case, the man in...  0.005402   \n",
       "1068546  1068546  https://www.youtube.com/watch?v=Qup47XJGIl4\\n\\...  0.003538   \n",
       "1068556  1068556  Liu Xiaobo is too famous. If he was able to tr...  0.018004   \n",
       "1068572  1068572  Well aren't you our Debbie Downer? You must be...  0.063338   \n",
       "1068584  1068584                             The postman was white?  0.045931   \n",
       "\n",
       "         severe_toxicity   obscene    threat    insult  identity_attack  \\\n",
       "11          2.657304e-07  0.000446  0.000147  0.002342         0.000222   \n",
       "22          1.868301e-06  0.001234  0.000561  0.471114         0.014231   \n",
       "37          5.262926e-07  0.000097  0.000148  0.000871         0.003759   \n",
       "59          1.169475e-07  0.000085  0.000058  0.000294         0.000052   \n",
       "66          4.219486e-07  0.000433  0.000190  0.001041         0.000182   \n",
       "...                  ...       ...       ...       ...              ...   \n",
       "1068545     6.007705e-08  0.000067  0.000152  0.002832         0.000166   \n",
       "1068546     1.102081e-07  0.000132  0.000490  0.000461         0.000075   \n",
       "1068556     3.734395e-07  0.000132  0.000294  0.006486         0.002575   \n",
       "1068572     9.793220e-08  0.000544  0.000183  0.060089         0.000293   \n",
       "1068584     1.260926e-06  0.000161  0.000323  0.005694         0.019249   \n",
       "\n",
       "         sexual_explicit  \n",
       "11              0.017254  \n",
       "22              0.000574  \n",
       "37              0.000157  \n",
       "59              0.000090  \n",
       "66              0.021162  \n",
       "...                  ...  \n",
       "1068545         0.000160  \n",
       "1068546         0.000177  \n",
       "1068556         0.000189  \n",
       "1068572         0.000346  \n",
       "1068584         0.000183  \n",
       "\n",
       "[131093 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>christian</th>\n",
       "      <th>white</th>\n",
       "      <th>muslim</th>\n",
       "      <th>black</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No, we need Trump to stay in so he can lose.</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Wow.  Precious.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Haven't been here when the oil economy dead ha...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>'\\nBaloney.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it hit and run, then the word \"accident\" ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068545</th>\n",
       "      <td>1068545</td>\n",
       "      <td>In a recent highly publicised case, the man in...</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>6.007705e-08</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068546</th>\n",
       "      <td>1068546</td>\n",
       "      <td>https://www.youtube.com/watch?v=Qup47XJGIl4\\n\\...</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>1.102081e-07</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068556</th>\n",
       "      <td>1068556</td>\n",
       "      <td>Liu Xiaobo is too famous. If he was able to tr...</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>3.734395e-07</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068572</th>\n",
       "      <td>1068572</td>\n",
       "      <td>Well aren't you our Debbie Downer? You must be...</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>9.793220e-08</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.060089</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068584</th>\n",
       "      <td>1068584</td>\n",
       "      <td>The postman was white?</td>\n",
       "      <td>0.045931</td>\n",
       "      <td>1.260926e-06</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.019249</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068585 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  toxicity  \\\n",
       "0              0       No, we need Trump to stay in so he can lose.  0.166667   \n",
       "1              1                                    Wow.  Precious.  0.000000   \n",
       "2              2  Haven't been here when the oil economy dead ha...  0.200000   \n",
       "3              3                                        '\\nBaloney.  0.000000   \n",
       "4              4  When it hit and run, then the word \"accident\" ...  0.000000   \n",
       "...          ...                                                ...       ...   \n",
       "1068545  1068545  In a recent highly publicised case, the man in...  0.005402   \n",
       "1068546  1068546  https://www.youtube.com/watch?v=Qup47XJGIl4\\n\\...  0.003538   \n",
       "1068556  1068556  Liu Xiaobo is too famous. If he was able to tr...  0.018004   \n",
       "1068572  1068572  Well aren't you our Debbie Downer? You must be...  0.063338   \n",
       "1068584  1068584                             The postman was white?  0.045931   \n",
       "\n",
       "         severe_toxicity   obscene    threat    insult  identity_attack  \\\n",
       "0           0.000000e+00  0.000000  0.000000  0.166667         0.000000   \n",
       "1           0.000000e+00  0.000000  0.000000  0.000000         0.000000   \n",
       "2           0.000000e+00  0.000000  0.200000  0.000000         0.000000   \n",
       "3           0.000000e+00  0.000000  0.000000  0.000000         0.000000   \n",
       "4           0.000000e+00  0.000000  0.000000  0.000000         0.000000   \n",
       "...                  ...       ...       ...       ...              ...   \n",
       "1068545     6.007705e-08  0.000067  0.000152  0.002832         0.000166   \n",
       "1068546     1.102081e-07  0.000132  0.000490  0.000461         0.000075   \n",
       "1068556     3.734395e-07  0.000132  0.000294  0.006486         0.002575   \n",
       "1068572     9.793220e-08  0.000544  0.000183  0.060089         0.000293   \n",
       "1068584     1.260926e-06  0.000161  0.000323  0.005694         0.019249   \n",
       "\n",
       "         sexual_explicit  female  male  christian  white  muslim  black  \\\n",
       "0               0.000000     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1               0.000000     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "2               0.000000     0.0   0.0        0.0    0.0     0.0    0.0   \n",
       "3               0.000000     0.0   0.0        0.0    0.0     0.0    0.0   \n",
       "4               0.000000     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "...                  ...     ...   ...        ...    ...     ...    ...   \n",
       "1068545         0.000160     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068546         0.000177     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068556         0.000189     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068572         0.000346     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "1068584         0.000183     NaN   NaN        NaN    NaN     NaN    NaN   \n",
       "\n",
       "         homosexual_gay_or_lesbian  jewish  psychiatric_or_mental_illness  \\\n",
       "0                              NaN     NaN                            NaN   \n",
       "1                              NaN     NaN                            NaN   \n",
       "2                              0.0     0.0                            0.0   \n",
       "3                              0.0     0.0                            0.0   \n",
       "4                              NaN     NaN                            NaN   \n",
       "...                            ...     ...                            ...   \n",
       "1068545                        NaN     NaN                            NaN   \n",
       "1068546                        NaN     NaN                            NaN   \n",
       "1068556                        NaN     NaN                            NaN   \n",
       "1068572                        NaN     NaN                            NaN   \n",
       "1068584                        NaN     NaN                            NaN   \n",
       "\n",
       "         asian  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "1068545    NaN  \n",
       "1068546    NaN  \n",
       "1068556    NaN  \n",
       "1068572    NaN  \n",
       "1068584    NaN  \n",
       "\n",
       "[1068585 rows x 19 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([train_d,subgroup],axis=0)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"make_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MultiLabelDataset(test, tokenizer, MAX_LEN, eval_mode = True)\n",
    "testing_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 4\n",
    "                }\n",
    "test_loader = DataLoader(test_set, **testing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_pred = []\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "    \n",
    "        for _, data in tqdm(enumerate(test_loader, 0)):\n",
    "\n",
    "\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            probas = torch.sigmoid(outputs)\n",
    "\n",
    "            all_test_pred.append(probas)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/anaconda3/envs/py31/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "13482it [32:31,  6.91it/s]\n"
     ]
    }
   ],
   "source": [
    "probas = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_pred = torch.cat(all_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['prediction'] = all_test_pred[:,0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.058943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.085496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431410</th>\n",
       "      <td>431410</td>\n",
       "      <td>0.192491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431411</th>\n",
       "      <td>431411</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431412</th>\n",
       "      <td>431412</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431413</th>\n",
       "      <td>431413</td>\n",
       "      <td>0.012632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431414</th>\n",
       "      <td>431414</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431415 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prediction\n",
       "0            0    0.000596\n",
       "1            1    0.003322\n",
       "2            2    0.000332\n",
       "3            3    0.058943\n",
       "4            4    0.085496\n",
       "...        ...         ...\n",
       "431410  431410    0.192491\n",
       "431411  431411    0.000388\n",
       "431412  431412    0.000331\n",
       "431413  431413    0.012632\n",
       "431414  431414    0.000513\n",
       "\n",
       "[431415 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submit_Epoch2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431410</th>\n",
       "      <td>431410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431411</th>\n",
       "      <td>431411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431412</th>\n",
       "      <td>431412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431413</th>\n",
       "      <td>431413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431414</th>\n",
       "      <td>431414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431415 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id\n",
       "0            0\n",
       "1            1\n",
       "2            2\n",
       "3            3\n",
       "4            4\n",
       "...        ...\n",
       "431410  431410\n",
       "431411  431411\n",
       "431412  431412\n",
       "431413  431413\n",
       "431414  431414\n",
       "\n",
       "[431415 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = submission.drop(columns='prediction')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(col):\n",
    "    sub[name] = all_test_pred[:,i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('subtype.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
